# Document Reader Tool

A simple web-based tool for reading PDF documents and asking questions about their content using LLM-powered Q&A.

## 📋 Description

This project implements a straightforward document reader that allows users to:

- **Upload PDF files**: Extract text content from PDF documents
- **Ask questions**: Query the document content using natural language
- **Get AI-powered answers**: Receive responses generated by LLMs based on the document content

## 🏗️ Architecture

```
PDF Upload → Text Extraction → Chunking → LLM Query → Response
```

### Main Components:

1. **`server.js`** - Express server with file upload and web interface
2. **`pdfReader.js`** - PDF text extraction using pdf-parse
3. **`chunker.js`** - Text chunking for processing large documents
4. **`llm.js`** - LLM integration via OpenRouter API
5. **`package.json`** - Project dependencies and configuration

## 🚀 Installation

### Prerequisites

- Node.js (version 14 or higher)
- npm or yarn

### Installation Steps

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd llmReadPdf
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Configure environment variables**
   ```bash
   cp .env.example .env
   ```

   Edit the `.env` file and add your OpenRouter API key:
   ```
   OPENROUTER_API_KEY=your_api_key_here
   ```

## ⚙️ Configuration

### Environment Variables

- `OPENROUTER_API_KEY`: Your OpenRouter API key to access LLM models

### Chunking Configuration

- **Chunk size**: 1500 words per chunk
- **Overlap**: No overlap between chunks
- **Validation**: Chunks are created by splitting text into word arrays

## 🎯 Usage

### Basic Execution

```bash
npm start
```

The server will start on `http://localhost:3000`. Open this URL in your browser to access the web interface.

### Workflow

1. **Access the UI**: Navigate to `http://localhost:3000` in your browser
2. **Upload PDF**: Select a PDF file using the file input
3. **Ask a question**: Enter your question about the document in the text field
4. **Get response**: Click "Process Document" to receive an AI-generated answer

### Example Usage

1. Upload `assets/prueba_tecnica_platzi.pdf`
2. Ask: "What is this document about?"
3. Receive a summary of the document content

## 📁 Project Structure

```
llmReadPdf/
├── server.js                        # Express server and web interface
├── pdfReader.js                     # PDF text extraction
├── chunker.js                       # Text chunking logic
├── llm.js                           # LLM API integration
├── package.json                     # Dependencies and scripts
├── .env                             # Environment variables
└── README.md                        # This file
```

## 🔧 Dependencies

### Main Dependencies

- **`express`**: Web server framework
- **`multer`**: File upload handling
- **`pdf-parse`**: PDF text extraction
- **`axios`**: HTTP client for API calls
- **`dotenv`**: Environment variable management

### External APIs

- **OpenRouter**: For LLM model access (openai/gpt-oss-20b:free)

## 🐛 Troubleshooting

### Error: "OpenRouter API key not configured"

- Verify that the `.env` file exists and contains the API key
- Make sure the environment variable is correctly set
- Restart the server after adding the API key

### Error: "No endpoints found for [model]"

- The specified LLM model may not be available
- Try using a different model like 'openai/gpt-3.5-turbo' or 'anthropic/claude-3-haiku'

### Error: "File not found"

- Verify that the PDF file exists in the specified path
- Check file read permissions

### Performance Issues

- Large PDFs may take time to process due to chunking and multiple LLM calls
- Consider reducing chunk size or implementing caching for repeated queries

## 🔒 Security

- API keys are managed through environment variables
- PDF content is processed server-side only
- No user data is stored permanently

## 📄 License

This project is under the ISC License. See the `package.json` file for more details.

---

**Note**: This project was developed as a technical test to demonstrate PDF processing and LLM integration capabilities.
