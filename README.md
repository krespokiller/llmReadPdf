# RAG PDF Summarizer

A web-based tool for reading PDF documents and asking questions about their content using local LLM and embedding models running in Docker.

## 📋 Description

This project implements a document reader with Domain-Driven Design (DDD) architecture that allows users to:

- **Upload PDF files**: Extract text content from PDF documents
- **Ask questions**: Query the document content using natural language
- **Get AI-powered answers**: Receive responses generated by a local LLM model based on the document content

## 🏗️ Architecture

The project follows Domain-Driven Design (DDD) principles with three bounded contexts implementing a complete RAG (Retrieval-Augmented Generation) system:

```
src/
├── domains/
│   ├── ui/                    # User Interface Domain
│   │   ├── controllers/       # HTTP request handlers
│   │   ├── views/             # HTML templates
│   │   └── routes/            # Express routes
│   ├── document-processing/   # Document Processing Domain
│   │   └── services/          # PDF reading, text cleaning, chunking, embeddings
│   └── llm/                   # LLM Domain
│       └── services/          # Local LLM integration
└── start.js                   # Application entry point
```

### RAG Pipeline:

1. **PDF Upload** → Text extraction
2. **Text Processing** → Cleaning and chunking
3. **Embedding Generation** → Generate embeddings for each chunk using a local embedding model
4. **Semantic Search** → Find most relevant chunks for user query
5. **Context Retrieval** → Combine relevant chunks
6. **LLM Query** → Generate answer using retrieved context

## 🚀 Installation

### Prerequisites

- Node.js (version 14 or higher)
- npm or yarn
- Docker with local model runners running
- Two local Docker models:
  - `embeddinggemma:latest` for embeddings
  - `deepseek-r1-distill-llama:latest` for LLM queries

### Installation Steps

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd llmReadPdf
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Configure environment variables**

    Create a `.env` file with:
    ```
    PORT=3000
    ```

4. **Start the Docker AI Models**


    ** Delete the models **
    ```bash
    docker model rm ai/embeddinggemma:latest
    docker model rm ai/deepseek-r1-distill-llama:latest
    ```

    ** Run the models, it download if are not available **
    ```bash
    docker model run ai/embeddinggemma:latest
    docker model run ai/deepseek-r1-distill-llama:latest
    ```

    ** Verify models and endpoints **
    ```bash
    docker model list
    ```

    ** Stop the models **
    ```bash
    docker model unload ai/embeddinggemma:latest
    docker model unload ai/deepseek-r1-distill-llama:latest
    ```



5. **Start the Node.js server**

    ```bash
    npm start
    ```

    The server will run on `http://localhost:3000`

## ⚙️ Configuration

### Local Model Configuration

- **LLM Model**: `ai/deepseek-r1-distill-llama:latest`
- **LLM Endpoint**: `http://localhost:12435/engines/llama.cpp/v1/chat/completions`
- **Embedding Model**: `ai/embeddinggemma:latest`
- **Embedding Endpoint**: `http://localhost:12434/engines/llama.cpp/v1/embeddings`
- **API Keys**: None required - all models run locally

### RAG Configuration

- **Semantic Search**: Uses cosine similarity between query and chunk embeddings
- **Top K Results**: Returns top 3 most relevant chunks
- **Context Window**: Combines relevant chunks for LLM context
- **Embedding Dimension**: 384 (embeddinggemma model)

## 🎯 Usage

### Basic Execution

```bash
npm start
```

The server will start on `http://localhost:3000`. Open this URL in your browser to access the web interface.

### Workflow

1. **Access the UI**: Navigate to `http://localhost:3000` in your browser
2. **Upload PDF**: Select a PDF file using the file input
3. **Ask a question**: Enter your question about the document in the text field
4. **Get response**: Click "Process Document" to receive an AI-generated answer

### Example Usage

1. Upload a PDF file
2. Ask: "What is this document about?"
3. Receive a summary of the document content

## 📁 Project Structure

```
llmReadPdf/
├── src/
│   ├── domains/
│   │   ├── ui/
│   │   │   ├── controllers/
│   │   │   │   └── DocumentController.js
│   │   │   ├── views/
│   │   │   │   └── index.html
│   │   │   └── routes/
│   │   │       └── documentRoutes.js
│   │   ├── document-processing/
│   │   │   ├── services/
│   │   │   │   ├── PdfReaderService.js
│   │   │   │   ├── TextCleanerService.js
│   │   │   │   └── ChunkerService.js
│   │   │   └── index.js
│   │   └── llm/
│   │       ├── services/
│   │       │   └── LlmQueryService.js
│   │       └── index.js
│   └── server.js
├── package.json
└── README.md
```

## 🔧 Dependencies

### Main Dependencies

- **`express`**: Web server framework
- **`multer`**: File upload handling
- **`pdf-parse`**: PDF text extraction
- **`axios`**: HTTP client for local model API calls
- **`dotenv`**: Environment variable management

### Docker AI Models Required

- **`ai/embeddinggemma:latest`**: Local embedding model (runs on port 12434)
- **`ai/deepseek-r1-distill-llama:latest`**: Local LLM model (runs on port 12435)

### Docker Model Runner Setup

The application uses Docker Model Runner to serve local AI models. Make sure you have Docker Desktop installed and running.

**Model Management Commands:**
```bash
# List available models
docker model list

# Run models (downloads if not available)
docker model run ai/embeddinggemma:latest
docker model run ai/deepseek-r1-distill-llama:latest

# Stop models
docker model unload ai/embeddinggemma:latest
docker model unload ai/deepseek-r1-distill-llama:latest

# Remove models
docker model rm ai/embeddinggemma:latest
docker model rm ai/deepseek-r1-distill-llama:latest
```

**API Endpoints:**
- Embedding Model: `http://localhost:12434/engines/llama.cpp/v1/embeddings`
- LLM Model: `http://localhost:12435/engines/llama.cpp/v1/chat/completions`

### Local Models

- **LLM Model**: `deepseek-r1-distill-llama:latest` running locally in Docker
- **Embedding Model**: `embeddinggemma:latest` running locally in Docker

## 🐛 Troubleshooting

### Error: "Embedding generation error"

- Verify that your local `ai/embeddinggemma:latest` Docker model is running on port 12434
- Check that the endpoint `http://localhost:12434/engines/llama.cpp/v1/embeddings` is accessible
- Ensure the model `ai/embeddinggemma:latest` is available in your Docker setup
- Check the application logs for detailed error information including API request/response details

### Error: "Local LLM API error"

- Verify that your local `ai/deepseek-r1-distill-llama:latest` Docker model is running on port 12435
- Check that the endpoint `http://localhost:12435/engines/llama.cpp/v1/chat/completions` is accessible
- Ensure the model `ai/deepseek-r1-distill-llama:latest` is available in your Docker setup
- Check the application logs for detailed error information

### Error: "Connection refused"

- Make sure Docker is running
- Verify the model runner container is started
- Check that port 12434 is accessible

### Error: "File not found"

- Verify that the PDF file exists in the specified path
- Check file read permissions

### Performance Issues

- Large PDFs may take time to process due to chunking and multiple LLM calls
- Consider reducing chunk size or implementing caching for repeated queries

## 🔒 Security

- No external API keys required (all models run locally)
- PDF content is processed server-side only
- No user data is stored permanently
- All processing happens locally

## 📄 License

This project is under the ISC License. See the `package.json` file for more details.

---

**Note**: This project was developed as a technical test to demonstrate PDF processing and LLM integration capabilities.
